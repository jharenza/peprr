## Code to load data for ROA
#reads datafiles parse into dataframes and loads into sqlite DB

## TODO
# function for parsing input filenames to get accession information

# initiating database
#' Initiates sqlite database for storing pepr pipeline output data.
#'
#' @param data_dir data directory generated by the pepr pipeline,
#' will generate new database in directory if db_path is not provided.
#' @param db_path optional path to initiate the database
#' @param Set to FALSE to have database be read-only, default TRUE
#' @return dplyr database ....
#' @examples
#' init_peprDB("/path/to/PEPR-DATA")
#' init_peprDB("/path/to/PEPR-DATA","/path/to/peprDB.sqlite")
init_peprDB <- function(data_dir = NULL, db_path = NULL, create = TRUE){
    # not working when only supplied db_path
    if(is.null(db_path)){
        db_path <- stringr::str_c(data_dir, "peprDB.sqlite", sep = "/")
    }else if(is.null(data_dir)){
        stop("Either data_dir or db_path are needed")
    }
    dplyr::src_sqlite(db_path, create = create)
}

#### Metadata
#' Loads metadata file from pepr pipeline into database
#' @param metadata yaml file with pipeline metadata
#' @param db_con optional path to initiate the database
#' @return database
#' @examples
#' load_peprMeta("/path/to/metadata.yaml", pepr_con)
load_peprMeta <- function(metadata, db_con){
    yList <-  yaml::yaml.load_file(metadata)
    # need to figure out what to do with general metadata ....
    ydf <- dplyr::data_frame()
    for(i in 1:length(yList[["exp_design"]])){
        subList <- yList[["exp_design"]][[i]]
        for(j in 1:length(subList)){
            sdf <- dplyr::as_data_frame(subList[[j]])
            ydf <- dplyr::bind_rows(ydf, sdf)
        }
    }
    dplyr::copy_to(dest = db_con,df = ydf,name = "exp_design")
}

#### Summary of sequencing datasets ####
# parse stats file names
parse_stat_name <- function(file_name){
    name_split <- stringr::str_split(file_name,pattern = "/")[[1]]
    name_file <- length(name_split)
    stringr::str_split(name_split[length(name_split)],
                       pattern = "_stats")[[1]][1]
}

read_metrics <- function(metrics_file, nskip){
    m_df <- read.table(metrics_file,
                        skip = nskip, sep ="\t",
                       header =T, stringsAsFactors=F) %>%
        dplyr::mutate(accession = parse_stat_name(metrics_file))
    return(m_df)
}

map_met_df <- function(met_type, metrics_files){
    met_list <- grep(pattern = met_type[[1]], metrics_files,value = TRUE)
    plyr::ldply(.data = met_list,.fun = read_metrics, nskip = met_type[[2]])
}

#### Summary of sequencing datasets ####
#metrics and fastqc output
load_metrics <- function(metrics_dir, db_con){
    # alignment summary metrics
    metrics_files <- list.files(metrics_dir,"_stat",full.names = TRUE)
    metric_types <- list("align"=list("*alignment_summary_metrics", 6),
                       "quality"= list("*quality_distribution_metrics", 6),
                       "cycle"=list("*quality_by_cycle_metric", 6),
                       #"insert"=list("*insert_size_metric", 6),
                       # insert size metric is not working error:
                       # line 2 did not have 21 elements
                       "insert_hist"=list("*insert_size_metric", 10))
    met_df_list <- plyr::llply(metric_types, .f = map_met_df, metrics_files)
    for(i in names(met_df_list)){
        dplyr::copy_to(dest = db_con,df = met_df_list[[i]],name = i)
    }
}

#extract dataset name from file name
.get_dataset_name <- function(fastqc_data_file){
    split_dir_fqc <- stringr::str_split(fastqc_data_file,pattern = "/")[[1]]
    fqc_name <- grep(pattern = "*_fastqc", x= split_dir_fqc, value = TRUE) %>%
        stringr::str_replace(pattern = "_fastqc",replacement = "") %>%
        stringr::str_replace(pattern = "_1",replacement = "")
}

## loading fastqc results
load_fastqc <- function(metrics_dir, db_con){
    metrics_files <- list.files(metrics_dir,"fastqc_data.txt",
                                full.names = TRUE,recursive = TRUE)
    read_fastqc <- plyr::llply(metrics_files, readFastQC)
    names(read_fastqc) <-  plyr::llply(metrics_files, .get_dataset_name)

    for(i in c("Per_base_sequence_quality",
               "Per_sequence_quality_scores",
               "Sequence_Length_Distribution")){
        df <- plyr::ldply(read_fastqc,i) %>%
            dplyr::rename(accession=.id)
        dplyr::copy_to(dest = db_con,df = df,name = i)
    }
}

#### Purity ####
# pathoscope output
load_purity <- function(purity_dir, db_con){
    # need to test with pathoscope files ....
    purity_df <- plyr::ldply(list.files(purity_dir,pattern = "*sam-report.tsv",
                                  full.names = TRUE),
                       parse_sam_report)
    dplyr::copy_to(dest = db_con,df = purity_df,name = "purity")
}



#### Genome Assembly #####
# pilon changes
# pilon vcf
# pilon wig files

##### Base level analysis ####
#load full genome vcf
load_consensus <- function(con_base_dir, ref, db_con){
    # need to revisit only works for single sample inputs ...
    vcf_dir_list <- list.files(con_base_dir, full.names = TRUE)
    purrr::each(.x= vcf_dir_list,.f = process_vcf_purity,
                ref = ref, db_con = db_con)
}

#### Homogeneity ####
#load varscan results
load_varscan <- function(homogeneity_dir, db_con){
    # will potentially want to clean up the dataframe to trim off the % signs
    # and convert to numeric
    for(i in c("varscan-indel", "varscan-snp")){
        file_list <- list.files(homogeneity_dir,
                                pattern = paste0("*", i, "*"),full.names = TRUE)
        varscan_df <- purrr::map(.x = file_list, .f = .read_varscan) %>%
                        dplyr::rbind_all()
        dplyr::copy_to(dest = db_con,df = varscan_df,
                       name = stringr::str_replace(i, "-","_"))
    }
}
